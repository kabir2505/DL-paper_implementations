{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor,transforms\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset\n",
    "train_data=datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "                                    transforms.Resize((32,32)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean = (0.1307,), std = (0.3081,))]),\n",
    "    target_transform=None\n",
    ")\n",
    "\n",
    "test_data=datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose([\n",
    "                                    transforms.Resize((32,32)),\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(mean = (0.1307,), std = (0.3081,))])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image,label=train_data[0]\n",
    "image.shape #colo_channels=1,height,width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 60000, 10000, 10000)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.data),len(train_data.targets),len(test_data.data),len(test_data.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 32, 32])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = train_data[0]\n",
    "print(f\"Image shape: {image.shape}\")\n",
    "plt.imshow(image.squeeze()) #[color_channels,height,width]\n",
    "plt.title(label);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataloaders: (<torch.utils.data.dataloader.DataLoader object at 0x14ce314f0>, <torch.utils.data.dataloader.DataLoader object at 0x14ce31af0>)\n",
      "Length of train dataloader: 1875 batches of 32\n",
      "Length of test dataloader: 313 batches of 32\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE=32\n",
    "\n",
    "train_dataloader = DataLoader(train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              shuffle=True)\n",
    "\n",
    "test_dataloader=DataLoader(test_data,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           shuffle=False\n",
    "\n",
    ")\n",
    "print(f\"Dataloaders: {train_dataloader, test_dataloader}\") \n",
    "print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\n",
    "print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5ModelV1(nn.Module):\n",
    "    \"\"\"Model architecture\"\"\"\n",
    "\n",
    "    def __init__(self,input_shape:int,output_shape:int):\n",
    "        super().__init__()\n",
    "        self.layer_1=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,out_channels=6,kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "        )\n",
    "        self.layer_2=nn.Sequential(\n",
    "            nn.Conv2d(in_channels=6,out_channels=16,kernel_size=5),\n",
    "            nn.Tanh(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fc1=nn.Linear(in_features=400,out_features=120)\n",
    "        self.relu1=nn.ReLU()\n",
    "        self.fc2=nn.Linear(in_features=120,out_features=84)\n",
    "        self.relu2=nn.ReLU()\n",
    "\n",
    "        self.output=nn.Linear(84,out_features=output_shape)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \n",
    "        out=self.layer_2(self.layer_1(x))\n",
    "        #flatten the layer before fc1\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out=self.output(self.relu2(self.fc2(self.relu1(self.fc1(out)))))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LeNet5ModelV1(input_shape=1,output_shape=10).to(device)\n",
    "\n",
    "#loss\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    \"\"\"Calculates accuracy between truth labels and predictions.\"\"\"\n",
    "\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.25048 | Test loss: 0.17712, Test acc: 95.35%\n",
      "\n",
      "Epoch: 1\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.16451 | Test loss: 0.15797, Test acc: 96.12%\n",
      "\n",
      "Epoch: 2\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.14475 | Test loss: 0.12888, Test acc: 97.20%\n",
      "\n",
      "Epoch: 3\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.13139 | Test loss: 0.14260, Test acc: 96.99%\n",
      "\n",
      "Epoch: 4\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.12961 | Test loss: 0.14680, Test acc: 96.92%\n",
      "\n",
      "Epoch: 5\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.12310 | Test loss: 0.13200, Test acc: 96.99%\n",
      "\n",
      "Epoch: 6\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.10756 | Test loss: 0.11772, Test acc: 97.80%\n",
      "\n",
      "Epoch: 7\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.12098 | Test loss: 0.13634, Test acc: 97.35%\n",
      "\n",
      "Epoch: 8\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.10047 | Test loss: 0.15034, Test acc: 97.16%\n",
      "\n",
      "Epoch: 9\n",
      "-------\n",
      "Looked at 0/60000 samples\n",
      "Looked at 12800/60000 samples\n",
      "Looked at 25600/60000 samples\n",
      "Looked at 38400/60000 samples\n",
      "Looked at 51200/60000 samples\n",
      "\n",
      "Train loss: 0.11458 | Test loss: 0.13520, Test acc: 97.47%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "epochs=10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch}\\n-------\")\n",
    "    train_loss = 0\n",
    "    for batch,(X,y) in enumerate(train_dataloader):\n",
    "        #convert the data from cpu to target-device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        ###Training\n",
    "        model.train()\n",
    "        #1.forward pass\n",
    "        y_pred=model(X)\n",
    "        #2.calculate the loss\n",
    "        loss=loss_fn(y_pred,y)\n",
    "        train_loss+=loss\n",
    "        #3.optimizer zero_grad\n",
    "        optimizer.zero_grad()\n",
    "        #4.backprop\n",
    "        loss.backward()\n",
    "        #5.optimizer step\n",
    "        optimizer.step()\n",
    "        if batch % 400 == 0:\n",
    "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
    "    \n",
    "    train_loss /= len(train_dataloader)\n",
    "\n",
    "    test_loss, test_acc = 0, 0 \n",
    "    ###Testing\n",
    "    model.eval()\n",
    "    ##with torch no_grad\n",
    "    with torch.inference_mode():\n",
    "        for X, y in test_dataloader:\n",
    "            #target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            # 1. Forward pass\n",
    "            test_pred = model(X)\n",
    "           \n",
    "            # 2. Calculate loss (accumulatively)\n",
    "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
    "\n",
    "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
    "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
    "        \n",
    "        \n",
    "        # Divide total test loss by length of test dataloader (per batch)\n",
    "        test_loss /= len(test_dataloader)\n",
    "\n",
    "        # Divide total accuracy by length of test dataloader (per batch)\n",
    "        test_acc /= len(test_dataloader)\n",
    "\n",
    "    \n",
    "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'LeNet5ModelV1',\n",
       " 'model_loss': 0.11280898004770279,\n",
       " 'model_acc': 97.85343450479233}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "def eval_model(model: torch.nn.Module, \n",
    "               data_loader: torch.utils.data.DataLoader, \n",
    "               loss_fn: torch.nn.Module, \n",
    "               accuracy_fn):\n",
    "    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\"\"\"\n",
    "\n",
    "    \n",
    "    loss, acc = 0, 0\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        for X, y in data_loader:\n",
    "            #target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            #forward pass\n",
    "            y_pred = model(X)\n",
    "            \n",
    "            # Accumulate the loss and accuracy values per batch\n",
    "            loss += loss_fn(y_pred, y)\n",
    "            acc += accuracy_fn(y_true=y, \n",
    "                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -> pred_prob -> pred_labels)\n",
    "        \n",
    "        # Scale loss and acc to find the average loss/acc per batch\n",
    "        loss /= len(data_loader)\n",
    "        acc /= len(data_loader)\n",
    "        \n",
    "    return {\"model_name\": model.__class__.__name__, \n",
    "            \"model_loss\": loss.item(),\n",
    "            \"model_acc\": acc}\n",
    "\n",
    "# Calculate model 0 results on test dataset\n",
    "model_0_results = eval_model(model=model, data_loader=test_dataloader,\n",
    "    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n",
    ")\n",
    "model_0_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
